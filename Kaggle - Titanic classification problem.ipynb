{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic classification problem (https://www.kaggle.com/c/titanic)\n",
    "**The aim is to predict who, among the crew and passengers of the famous Titanic, survived the shipwreck, based on a series of predictors including age, ticket class, number of people in the family, sex, etc.\n",
    "In this code, I will select and engineer the features to be used and I will test the accuracy of four algorithms: logistic regression, neural networks, random forests and extreme gradient boosting **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input parameters:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = 700 # number of training samples\n",
    "num_cv = 191    # number of samples for cross-validation\n",
    "num_test = 418  # number of test samples\n",
    "\n",
    "num_runs = 10   # number of runs for cross-validation\n",
    "\n",
    "train_file = \"train.csv\"\n",
    "test_file = \"test.csv\"\n",
    "output_file = \"prediction.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = pd.read_csv(train_file, sep=',', index_col = 'PassengerId')  # train data\n",
    "te = pd.read_csv(test_file, sep=',', index_col = 'PassengerId')  # test data\n",
    "\n",
    "Id = te.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print first 5 lines of dataframe with training data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived  Pclass  \\\n",
      "PassengerId                     \n",
      "1                   0       3   \n",
      "2                   1       1   \n",
      "3                   1       3   \n",
      "4                   1       1   \n",
      "5                   0       3   \n",
      "\n",
      "                                                          Name     Sex   Age  \\\n",
      "PassengerId                                                                    \n",
      "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
      "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
      "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
      "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
      "5                                     Allen, Mr. William Henry    male  35.0   \n",
      "\n",
      "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
      "PassengerId                                                          \n",
      "1                1      0         A/5 21171   7.2500   NaN        S  \n",
      "2                1      0          PC 17599  71.2833   C85        C  \n",
      "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "4                1      0            113803  53.1000  C123        S  \n",
      "5                0      0            373450   8.0500   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "print(x.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print first 5 lines of dataframe with test data (note absence of column \"Survived\" as this is what needs to be predicted in this competition):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Pclass                                          Name     Sex  \\\n",
      "PassengerId                                                                 \n",
      "892               3                              Kelly, Mr. James    male   \n",
      "893               3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "894               2                     Myles, Mr. Thomas Francis    male   \n",
      "895               3                              Wirz, Mr. Albert    male   \n",
      "896               3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "              Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "PassengerId                                                       \n",
      "892          34.5      0      0   330911   7.8292   NaN        Q  \n",
      "893          47.0      1      0   363272   7.0000   NaN        S  \n",
      "894          62.0      0      0   240276   9.6875   NaN        Q  \n",
      "895          27.0      0      0   315154   8.6625   NaN        S  \n",
      "896          22.0      1      1  3101298  12.2875   NaN        S  \n"
     ]
    }
   ],
   "source": [
    "print(te.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection and engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop 'Ticket' column as not useful for prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.drop('Ticket', axis=1, inplace=True) # drop 'Ticket' column as not useful for prediction\n",
    "te.drop('Ticket', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign number to categorical variable 'Sex':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x['Sex'] = x['Sex'].astype('category')\n",
    "x['Sex'] = x['Sex'].cat.codes\n",
    "te['Sex'] = te['Sex'].astype('category')\n",
    "te['Sex'] = te['Sex'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process 'Age' column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace missing ages with -5 as it makes no sense to assign to mean age value\n",
    "x.loc[pd.isnull(x['Age']), 'Age'] = -5\n",
    "te.loc[pd.isnull(te['Age']), 'Age'] = -5\n",
    "\n",
    "# Set to 65 all ages above 65 (as very limited counts above 65)\n",
    "x.loc[x['Age']>65, 'Age'] = 65\n",
    "te.loc[te['Age']>65, 'Age'] = 65\n",
    "\n",
    "# Place ages in age groups (to decrease noise)\n",
    "labels = [ \"{0} - {1}\".format(\"%02d\" % i, \"%02d\" % (i + 4)) for i in range(-5, 70, 5) ]\n",
    "\n",
    "x['Age group'] = pd.cut(x['Age'], range(-5, 75, 5), right=False, labels=labels)\n",
    "te['Age group'] = pd.cut(te['Age'], range(-5, 75, 5), right=False, labels=labels)\n",
    "\n",
    "x.drop('Age', axis=1, inplace=True)\n",
    "te.drop('Age', axis=1, inplace=True)\n",
    "\n",
    "# Convert categorical values to numerical (dummy variable)\n",
    "x['Age group'] = x['Age group'].cat.codes\n",
    "te['Age group'] = te['Age group'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process columns 'SibSp' (number of siblings/spouses aboard the Titanic) and 'Parch' (number of parents/children aboard the Titanic):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace SibSp and Parch with their sum to find number of members in family, which is a better predictor\n",
    "x['Family'] = x['SibSp'] + x['Parch']\n",
    "te['Family'] = te['SibSp'] + te['Parch']\n",
    "\n",
    "x.loc[x['Family']>4, 'Family'] = 4 # put together all large families (because there are only few of them)\n",
    "te.loc[te['Family']>4, 'Family'] = 4\n",
    "\n",
    "x.drop('SibSp', axis=1, inplace=True)\n",
    "x.drop('Parch', axis=1, inplace=True)\n",
    "te.drop('SibSp', axis=1, inplace=True)\n",
    "te.drop('Parch', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process columns 'Fare' and 'Embarked':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace one missing value for 'Fare' in test set with average among train set\n",
    "te.loc[pd.isnull(te['Fare']), 'Fare'] = x['Fare'].mean()\n",
    "\n",
    "# One-hot encoding for categorical variable 'Embarked'\n",
    "x = pd.get_dummies( x, columns=['Embarked'], drop_first=True )\n",
    "te = pd.get_dummies( te, columns=['Embarked'], drop_first=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process column 'Name' to extract the title and use it as a predictor for survival (Mr, Mrs, Miss, etc.):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract title from 'Name'\n",
    "x.loc[x['Name'].str.contains('Mr. '), 'Name'] = 'Mr'\n",
    "x.loc[x['Name'].str.contains('Mrs. '), 'Name'] = 'Mrs'\n",
    "x.loc[x['Name'].str.contains('Miss. '), 'Name'] = 'Miss'\n",
    "x.loc[x['Name'].str.contains('Master. '), 'Name'] = 'Master'\n",
    "x['Name'] = ['Other' if ((k != 'Mr') & (k != 'Mrs') & (k != 'Miss') & (k != 'Master')) else k for k in x['Name']]\n",
    "\n",
    "te.loc[te['Name'].str.contains('Mr. '), 'Name'] = 'Mr'\n",
    "te.loc[te['Name'].str.contains('Mrs. '), 'Name'] = 'Mrs'\n",
    "te.loc[te['Name'].str.contains('Miss. '), 'Name'] = 'Miss'\n",
    "te.loc[te['Name'].str.contains('Master. '), 'Name'] = 'Master'\n",
    "te['Name'] = ['Other' if ((k != 'Mr') & (k != 'Mrs') & (k != 'Miss') & (k != 'Master')) else k for k in te['Name']]\n",
    "\n",
    "# One-hot encoding of title\n",
    "x = pd.get_dummies( x, columns=['Name'], drop_first=True )\n",
    "te = pd.get_dummies( te, columns=['Name'], drop_first=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Process column 'Cabin':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Substitute missing values for 'Cabin' with 'other'\n",
    "x.loc[pd.isnull(x['Cabin']), 'Cabin'] = 'other'\n",
    "te.loc[pd.isnull(te['Cabin']), 'Cabin'] = 'other'\n",
    "\n",
    "# Extract cabin letter from 'Cabin'\n",
    "x.loc[x['Cabin'].str.contains('A'), 'Cabin'] = 'A'\n",
    "x.loc[x['Cabin'].str.contains('B'), 'Cabin'] = 'B'\n",
    "x.loc[x['Cabin'].str.contains('C'), 'Cabin'] = 'C'\n",
    "x.loc[x['Cabin'].str.contains('D'), 'Cabin'] = 'D'\n",
    "x.loc[x['Cabin'].str.contains('E'), 'Cabin'] = 'E'\n",
    "x.loc[x['Cabin'].str.contains('F'), 'Cabin'] = 'F'\n",
    "x.loc[x['Cabin'].str.contains('G'), 'Cabin'] = 'F' # because very few counts in category G\n",
    "x['Cabin'] = ['Other' if ((k != 'A') & (k != 'B') & (k != 'C') & (k != 'D') & (k != 'E') & (k != 'F') & (k != 'G')) else k for k in x['Cabin']]\n",
    "\n",
    "te.loc[te['Cabin'].str.contains('A'), 'Cabin'] = 'A'\n",
    "te.loc[te['Cabin'].str.contains('B'), 'Cabin'] = 'B'\n",
    "te.loc[te['Cabin'].str.contains('C'), 'Cabin'] = 'C'\n",
    "te.loc[te['Cabin'].str.contains('D'), 'Cabin'] = 'D'\n",
    "te.loc[te['Cabin'].str.contains('E'), 'Cabin'] = 'E'\n",
    "te.loc[te['Cabin'].str.contains('F'), 'Cabin'] = 'F'\n",
    "te.loc[te['Cabin'].str.contains('G'), 'Cabin'] = 'F' # because very few counts in category G\n",
    "te['Cabin'] = ['Other' if ((k != 'A') & (k != 'B') & (k != 'C') & (k != 'D') & (k != 'E') & (k != 'F') & (k != 'G')) else k for k in te['Cabin']]\n",
    "\n",
    "# One-hot encoding of cabin letter\n",
    "x = pd.get_dummies( x, columns=['Cabin'], drop_first=True )\n",
    "te = pd.get_dummies( te, columns=['Cabin'], drop_first=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save 'Survived' column in new variable y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = x['Survived']\n",
    "x.drop('Survived', axis=1, inplace=True) # drop column 'Survived' from dataframe of predictors x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert x, te and y dataframes to numpy arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(x)\n",
    "te = np.array(te)\n",
    "y = np.array(y)\n",
    "y = np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using various machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize prediction accuracy for train set and cross-validation set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy_log = np.zeros(num_runs)\n",
    "cv_accuracy_log = np.zeros(num_runs)\n",
    "\n",
    "train_accuracy_neur = np.zeros(num_runs)\n",
    "cv_accuracy_neur = np.zeros(num_runs)\n",
    "\n",
    "train_accuracy_randfor = np.zeros(num_runs)\n",
    "cv_accuracy_randfor = np.zeros(num_runs)\n",
    "\n",
    "train_accuracy_xgboost = np.zeros(num_runs)\n",
    "cv_accuracy_xgboost = np.zeros(num_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction using logistic regression (using only linear features), neural networks, random forests and extreme gradient boosting. The train set is sampled randomly in order to create train and cross-validation sets. The first is used for training, the second for evaluating accuracy on unseen examples. This procedure is repeated 10 times, and the accuracies for every run are recorded.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for kk in range(num_runs):    \n",
    "        \n",
    "    ## select randomly training and cross validation datasets\n",
    "    x_tr, x_cv, y_tr, y_cv = train_test_split(x, y, test_size=num_cv/(num_train+num_cv), random_state=None)\n",
    "     \n",
    "    # normalize independent variables to mean = 0 and std = 1    \n",
    "    xtrNorm = np.zeros(x_tr.shape)\n",
    "    for k in range(x_tr.shape[1]):\n",
    "        xtrNorm[:,k] = (x_tr[:,k] - np.mean(x_tr[:,k])) / np.std(x_tr[:,k])\n",
    "    \n",
    "    xcvNorm = np.zeros(x_cv.shape)\n",
    "    for k in range(x_cv.shape[1]): \n",
    "        xcvNorm[:,k] = (x_cv[:,k] - np.mean(x_tr[:,k])) / np.std(x_tr[:,k])\n",
    "    \n",
    "    teNorm = np.zeros(te.shape)\n",
    "    for k in range(te.shape[1]):\n",
    "        teNorm[:,k] = (te[:,k] - np.mean(x_tr[:,k])) / np.std(x_tr[:,k])\n",
    "    \n",
    "    # Add intercept term to training, cross-validation and test data\n",
    "    x_tr = np.concatenate((np.ones((num_train,1)), xtrNorm), axis=1)\n",
    "    x_cv = np.concatenate((np.ones((num_cv,1)), xcvNorm), axis=1)\n",
    "    teNorm = np.concatenate((np.ones((num_test,1)), teNorm), axis=1)\n",
    "            \n",
    "    y_tr = np.squeeze(y_tr)\n",
    "    y_cv = np.squeeze(y_cv) \n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    ## Logistic regression using only linear features\n",
    "    ##########################################\n",
    "    \n",
    "    clf = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1,\n",
    "                             fit_intercept=False, intercept_scaling=1, class_weight=None,\n",
    "                             random_state=None, solver='liblinear', max_iter=100,\n",
    "                             multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "    clf.fit(x_tr, y_tr)\n",
    "    \n",
    "    pred_tr = clf.predict(x_tr)\n",
    "    pred_cv = clf.predict(x_cv)\n",
    "    \n",
    "    # Compute accuracy on training set\n",
    "    train_accuracy_log[kk] = np.mean(pred_tr == y_tr) * 100\n",
    "    \n",
    "    # Compute accuracy on cross-validation set\n",
    "    cv_accuracy_log[kk] = np.mean(pred_cv == y_cv) * 100\n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    ## Neural network\n",
    "    ##########################################\n",
    "    \n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=17, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "    #clf = MLPClassifier(solver='lbfgs', alpha=16, hidden_layer_sizes=(8, 5), random_state=1)\n",
    "    #clf = MLPClassifier(solver='lbfgs', alpha=7, hidden_layer_sizes=(6), random_state=1)\n",
    "   \n",
    "    clf.fit(x_tr, y_tr)\n",
    "    \n",
    "    pred_tr = clf.predict(x_tr)\n",
    "    pred_cv = clf.predict(x_cv)\n",
    "    \n",
    "    # Compute accuracy on training set\n",
    "    train_accuracy_neur[kk] = np.mean(pred_tr == y_tr) * 100\n",
    "    \n",
    "    # Compute accuracy on cross-validation set\n",
    "    cv_accuracy_neur[kk] = np.mean(pred_cv == y_cv) * 100 \n",
    "\n",
    "\n",
    "    ##########################################\n",
    "    ## Random forest\n",
    "    ##########################################\n",
    "    \n",
    "#    clf = RandomForestClassifier(n_estimators=10, criterion=’gini’, max_depth=None, min_samples_split=2,\n",
    "#                           min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’,\n",
    "#                           max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "#                           bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0,\n",
    "#                           warm_start=False, class_weight=None)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,\n",
    "                           min_samples_leaf=3, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                           max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None,\n",
    "                           verbose=0, warm_start=False, class_weight=None)\n",
    "    clf.fit(x_tr, y_tr)\n",
    "    \n",
    "    pred_tr = clf.predict(x_tr)\n",
    "    pred_cv = clf.predict(x_cv)\n",
    "    \n",
    "    # Compute accuracy on training set\n",
    "    train_accuracy_randfor[kk] = np.mean(pred_tr == y_tr) * 100\n",
    "    \n",
    "    # Compute accuracy on cross-validation set\n",
    "    cv_accuracy_randfor[kk] = np.mean(pred_cv == y_cv) * 100 \n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    ## xgboost\n",
    "    ##########################################\n",
    "    \n",
    "    #clf = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\n",
    "    clf = XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.015, gamma=0.5)\n",
    "    clf.fit(x_tr, y_tr)\n",
    "    \n",
    "    pred_tr = clf.predict(x_tr)\n",
    "    pred_cv = clf.predict(x_cv)\n",
    "    \n",
    "    # Compute accuracy on training set\n",
    "    train_accuracy_xgboost[kk] = np.mean(pred_tr == y_tr) * 100\n",
    "    \n",
    "    # Compute accuracy on cross-validation set\n",
    "    cv_accuracy_xgboost[kk] = np.mean(pred_cv == y_cv) * 100 \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print results of calculation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average train accuracy for logistic regression: 83.300000\n",
      "with standard deviation of the mean: 0.306694\n",
      "\n",
      "Average cross validation accuracy for logistic regression: 82.094241\n",
      "with standard deviation of the mean: 1.014681\n",
      "\n",
      "\n",
      "Average train accuracy for neural network: 83.500000\n",
      "with standard deviation of the mean: 0.183225\n",
      "\n",
      "Average cross validation accuracy for neural network: 81.727749\n",
      "with standard deviation of the mean: 0.880609\n",
      "\n",
      "\n",
      "Average train accuracy for random forests: 87.600000\n",
      "with standard deviation of the mean: 0.311547\n",
      "\n",
      "Average cross validation accuracy for random forests: 82.460733\n",
      "with standard deviation of the mean: 0.895426\n",
      "\n",
      "\n",
      "Average train accuracy for xgb: 86.642857\n",
      "with standard deviation of the mean: 0.231455\n",
      "\n",
      "Average cross validation accuracy for xgb: 83.141361\n",
      "with standard deviation of the mean: 0.665560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display mean accuracies and their standard deviations for logistic regression\n",
    "print(\"\\nAverage train accuracy for logistic regression: %f\" % np.mean(train_accuracy_log))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(train_accuracy_log) / np.sqrt(num_runs)))\n",
    "print(\"Average cross validation accuracy for logistic regression: %f\" % np.mean(cv_accuracy_log))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(cv_accuracy_log) / np.sqrt(num_runs)))\n",
    "\n",
    "# display mean accuracies and their standard deviations for neural network\n",
    "print(\"\\nAverage train accuracy for neural network: %f\" % np.mean(train_accuracy_neur))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(train_accuracy_neur) / np.sqrt(num_runs)))\n",
    "print(\"Average cross validation accuracy for neural network: %f\" % np.mean(cv_accuracy_neur))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(cv_accuracy_neur) / np.sqrt(num_runs)))\n",
    "\n",
    "# display mean accuracies and their standard deviations for random forests\n",
    "print(\"\\nAverage train accuracy for random forests: %f\" % np.mean(train_accuracy_randfor))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(train_accuracy_randfor) / np.sqrt(num_runs)))\n",
    "print(\"Average cross validation accuracy for random forests: %f\" % np.mean(cv_accuracy_randfor))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(cv_accuracy_randfor) / np.sqrt(num_runs)))\n",
    "\n",
    "# display mean accuracies and their standard deviations for extreme gradient boosting\n",
    "print(\"\\nAverage train accuracy for xgb: %f\" % np.mean(train_accuracy_xgboost))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(train_accuracy_xgboost) / np.sqrt(num_runs)))\n",
    "print(\"Average cross validation accuracy for xgb: %f\" % np.mean(cv_accuracy_xgboost))\n",
    "print(\"with standard deviation of the mean: %f\\n\" % (np.std(cv_accuracy_xgboost) / np.sqrt(num_runs))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The best performance (84% average accuracy for cross-validation set) is obtained with the extreme gradient boosting algorithm, although the difference compared to the other methods is not statistically significant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_te = clf.predict(teNorm)\n",
    "#np.savetxt(output_file, p_te, delimiter=\",\")\n",
    "\n",
    "#create predictions dataframe in pandas\n",
    "p_te = np.squeeze(p_te)\n",
    "diction = {'PassengerId':Id, 'Survived':p_te.tolist()}\n",
    "predictions = pd.DataFrame(diction)\n",
    "\n",
    "predictions.to_csv(output_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
